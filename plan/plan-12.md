# 実行計画 12: スクレイピング基盤実装

## ゴール
- Netkeiba/JRA/地方競馬などの公開情報をスクレイピングし、レースデータベースへ取り込む処理を実装する。

## 手順
1. `backend/app/scraping/` ディレクトリを作成し、サイト別にスクレイパーモジュール（`netkeiba.py`, `jra.py`, `local_keiba.py`）を用意。
2. `requests`, `httpx`, `beautifulsoup4`, `selectolax`, `pydantic`（データシリアライズ）等の依存を追加。
3. robots.txtと利用規約を確認し、アクセス間隔やUser-Agentなどの遵守事項を設定ファイルにまとめる。
4. スクレイピングで取得した生データを正規化する `schemas` を作成し、DB保存用DTOに変換。
5. 失敗時のリトライ、レート制限（`asyncio` + `asyncio.Semaphore` や `tenacity`）を実装。
6. スクレイピング結果をDBへ保存するサービスを `backend/app/services/data_importer.py` に実装し、重複排除や更新日チェックを行う。
7. `pytest` でHTMLスナップショット（ローカル保存）を使ったパーサーテストを実施し、DOM変更に気づけるようにする。

## 成果物
- サイト別スクレイパーと共通インフラ（リトライ、ログ、プロキシ設定）。
- データベースにレース情報を投入するコマンド（`python -m app.scripts.fetch_races`など）。
- スクレイピングテストが通過し、主要パターンのHTMLがFixturesとして保存。

## 依存・前提
- 計画06のレース関連テーブルが存在し、保存先が決まっている。
- 外部アクセスが許可されているネットワーク環境。

## リスクと対策
- **リスク**: サイト構造変更。  
  **対策**: DOM要素の存在チェックとアラート、パーサーテストをCIに組み込み。
- **リスク**: アクセス制限やブロック。  
  **対策**: リクエスト間隔制御、キャッシュ利用、必要に応じAPI代替を検討。

