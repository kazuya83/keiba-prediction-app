# 実行計画 16: ML推論サービスとAPI統合

## ゴール
- 学習済みモデルの推論用マイクロサービスを実装し、バックエンドAPIから呼び出せるようにする。

## 手順
1. `ml/inference/` に推論サーバ（FastAPI or Flask）を作成し、`/infer` エンドポイントで特徴量入力→予測結果を返却。
2. モデル成果物（計画15で生成）をロードし、前処理（特徴量スケーリング、エンコード）をサーバ内に実装。
3. Dockerfile（マルチステージビルド）を作成し、推論サーバのコンテナイメージをビルド可能にする。
4. `docker-compose.yml` に `ml-inference` サービスを追加し、APIから内部ネットワークでアクセスできるように設定。
5. バックエンドの `prediction_runner`（計画09）でHTTP or gRPCクライアントを実装し、推論サーバと通信。
6. レイテンシ測定と負荷テスト（Locust等）で1分以内の応答を満たすか確認。必要ならバッチ推論/キューを採用。
7. CIで推論サーバのビルド・ユニットテストを実行するWorkflowを追加。

## 成果物
- 推論サーバコード、Dockerfile、`docker-compose` 設定。
- バックエンドから推論サーバを呼び出す統合コード。
- レイテンシ測定レポートと改善策。

## 依存・前提
- 計画15でモデル成果物が出力済みで、バージョン管理が可能。
- バックエンド予測API（計画09）が統合ポイントを用意している。

## リスクと対策
- **リスク**: モデルバージョンの不整合。  
  **対策**: 推論サーバでモデルバージョンをエンドポイントに返し、API側でチェック。
- **リスク**: 推論サーバのスケール不足。  
  **対策**: 将来的にKubernetes等でのスケールアウトを想定した構成にする。

